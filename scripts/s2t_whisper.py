
import time
import os
import wave
import pyaudio

import streamlit as st

from pathlib import Path
from openai import OpenAI

from .modules.audio_parameter import (
    CHUNK              ,
    FORMAT             ,
    SAMPLE_RATE        ,
    CHANNELS           ,
    INPUT_DEVICE_INDEX ,
    CALL_BACK_FREQUENCY,
)

from .utils_streamlit import (
    change_mic_state_to_disabled, 
    change_recording_state_to_true
)

client = OpenAI(
    api_key=os.environ["OPENAI_API_KEY"]
)

# ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
OUTPUT_TXT_FILE = Path(__file__).parent.parent / "data" / "user_speech.txt"
AUDIO_FILE_PATH = Path(__file__).parent.parent / "sounds" / "output.wav"



def record_audio():
    
    with wave.open('sounds/output.wav', 'wb') as wf:
        
        p = pyaudio.PyAudio()
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(SAMPLE_RATE)

        stream = p.open(format=FORMAT, channels=CHANNELS, rate=SAMPLE_RATE, input=True)

        print('Recording...')
        
        placeholder = st.empty()
    
        key_suffix  = 0
        while True:
            
            # stream ã‹ã‚‰èª­ã¿è¾¼ã¿å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
            in_data = stream.read(CHUNK)
            wf.writeframes(in_data)

            # ã“ã“ã‹ã‚‰ã¯æ¶ˆã™äºˆå®š ## ## ##
            # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«éŒ²éŸ³åœæ­¢ãƒœã‚¿ãƒ³ã‚’é…ç½®
            # with stop_button_placeholder.container():
                
            #     is_stop = st.button(
            #         label    = "Recording Stop ğŸ¤",
            #         key      = f"mic_stop_{key_suffix}",
            #         on_click = change_recording_state_to_true
            #     ) 
            
            if st.session_state.is_stop_recording : break
            
            placeholder.write(f"Recording... {key_suffix}")
            ## ## ## ã“ã“ã¾ã§ ## ## ## 
            
            # è¤‡è£½ç¦æ­¢ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ã®ã‚’é˜²ããŸã‚ã« key_suffix ã®æ›´æ–°
            key_suffix += 1
            
            # ãƒœã‚¿ãƒ³ç”Ÿæˆã‚’æŠ‘ãˆã‚‹ãŸã‚ã« 1 ç§’å¾…æ©Ÿï¼ˆæ¶ˆå»äºˆå®šï¼‰
            # time.sleep(1)
                
        print('Done')

        stream.close()
        p.terminate()


# éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹
def speech_2_text():
    
    record_audio()
    
    audio_file = open(AUDIO_FILE_PATH, "rb")
    transcript = client.audio.transcriptions.create(
        model           = "whisper-1", 
        file            = audio_file,
        language        = "en",
        response_format = "text"
    )
    
    # éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ã—ãŸã®ã¡ï¼Œãƒœã‚¿ãƒ³ã‚’ able ã«
    change_mic_state_to_disabled(disabled=False)
    
    return transcript


def main():
    speech_2_text()


if __name__ == '__main__':
    main()